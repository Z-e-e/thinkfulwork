{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Challenge\n",
    "Do a little scraping or API-calling of your own. Pick a new website and see what you can get out of it. Expect that you'll run into bugs and blind alleys, and rely on your mentor to help you get through.\n",
    "\n",
    "Formally, your goal is to write a scraper that will:\n",
    "\n",
    "1) Return specific pieces of information (rather than just downloading a whole page)<br/>\n",
    "2) Iterate over multiple pages/queries<br/>\n",
    "3) Save the data to your computer<br/>\n",
    "\n",
    "Once you have your data, compute some statistical summaries and/or visualizations that give you some new insights into your scraping topic of interest. Write up a report from scraping code to summary and share it with your mentor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraper - non-recursive/ one-page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install scrapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    }
   ],
   "source": [
    "import scrapy\n",
    "import re\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "class ESSpider(scrapy.Spider):\n",
    "    name = \"spider_13\"\n",
    "    \n",
    "    start_urls = [\n",
    "        'http://quotes.toscrape.com/',\n",
    "    ]\n",
    "\n",
    "    # Use XPath to parse the response we get.\n",
    "    def parse(self, response):\n",
    "        \n",
    "        # Iterate over every <article> element on the page.\n",
    "        #  response.xpath(\"//div[@class='quote']\").extract()\n",
    "\n",
    "        for div in response.xpath(\"//div[@class='quote']\"): #where to start before clicking on downward arrow\n",
    "            \n",
    "            # Yield a dictionary with the values we want.\n",
    "            yield {\n",
    "\n",
    "                'body': div.xpath(\"./span[@class='text']/text()\").extract_first(),\n",
    "                'author': div.xpath(\".//small[@class='author']/a/text()\").extract(),\n",
    "                'tags': div.xpath('*.//div[@class=\"tag\"]/a[@class=\"tag\"]/text()').extract()\n",
    "            }\n",
    "\n",
    "            \n",
    "# 'tags': quote.xpath('.//div[@class=\"tags\"]/a[@class=\"tag\"]/text()').extract()\n",
    "\n",
    "# Tell the script how to run the crawler by passing in settings.\n",
    "process = CrawlerProcess({\n",
    "    'FEED_FORMAT': 'json',         # Store data in JSON format.\n",
    "    'FEED_URI': 'trail_5.json',        # Name our storage file.\n",
    "    'LOG_ENABLED': False           # Turn off logging for now.\n",
    "})\n",
    "\n",
    "# Start the crawler with our spider.\n",
    "process.crawl(ESSpider)\n",
    "process.start()\n",
    "print('Success!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-7-4806be422c44>, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-7-4806be422c44>\"\u001b[1;36m, line \u001b[1;32m6\u001b[0m\n\u001b[1;33m    print(firstpage.head())\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "#pandas.read_json(open(\"ut1.json\", \"r\", encoding=\"utf8\"))\n",
    "\n",
    "firstpage = pd.read_json('trial_1.json',lines=False,orient='records')\n",
    "print(firstpage.shape)\n",
    "print(firstpage.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.xpath(\"//div[@class='quote']\").extract()\n",
    "response.xpath(\"//div[@class='quote']/span[@class='text']\").extract()\n",
    "response.xpath(\"//div[@class='quote']/span[@class='text']/text()\").extract()\n",
    "\n",
    "\n",
    "\n",
    "                'text': article.xpath('section[@class=\"entry-content\"]/p/text()').extract(),\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
